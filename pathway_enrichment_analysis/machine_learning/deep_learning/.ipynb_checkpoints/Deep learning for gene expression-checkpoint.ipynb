{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train dim:  (12778, 159)\n",
      "Y train dim:  (12778,)\n",
      "X validation dim:  (1694, 159)\n",
      "Y validation dim:  (1694,)\n",
      "X test dim:  (2117, 159)\n",
      "Y test dim:  (2117,)\n"
     ]
    }
   ],
   "source": [
    "#read the data\n",
    "#X_train = np.loadtxt(\"x_train_deep.txt\", delimiter=\"\\t\")\n",
    "#Y_train = np.loadtxt(\"y_train_deep.txt\", delimiter=\"\\t\")\n",
    "\n",
    "#OVERSAMPLING\n",
    "X_train = np.loadtxt(\"x_train_deep_over.txt\", delimiter=\"\\t\")\n",
    "Y_train = np.loadtxt(\"y_train_deep_over.txt\", delimiter=\"\\t\")\n",
    "\n",
    "X_validation = np.loadtxt(\"x_validation_deep.txt\", delimiter=\"\\t\")\n",
    "Y_validation = np.loadtxt(\"y_validation_deep.txt\", delimiter=\"\\t\")\n",
    "#Y_validation = np.loadtxt(\"y_validation_deep_onehot.txt\", delimiter=\"\\t\")\n",
    "\n",
    "X_test = np.loadtxt(\"x_test.txt\", delimiter=\"\\t\")\n",
    "Y_test = np.loadtxt(\"y_test.txt\", delimiter=\"\\t\")\n",
    "#Y_test = np.loadtxt(\"y_test_onehot.txt\", delimiter=\"\\t\")\n",
    "\n",
    "print(\"X train dim: \", X_train.shape)\n",
    "print(\"Y train dim: \", Y_train.shape)\n",
    "print(\"X validation dim: \", X_validation.shape)\n",
    "print(\"Y validation dim: \", Y_validation.shape)\n",
    "print(\"X test dim: \", X_test.shape)\n",
    "print(\"Y test dim: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make things reproducible\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "# HyperParameters\n",
    "training_epochs = 50\n",
    "display_step = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.8 #dropout \n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "#n_hidden_3 = 256\n",
    "num_input = 159 # MACCS descriptors without 7 features\n",
    "num_classes = 2 #  total classes (Inducer/not-Inducer)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, 1]) \n",
    "global_step = tf.Variable(name='step', dtype = tf.int32, initial_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    #'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 1]))\n",
    "    #'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    #'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([1]))\n",
    "    #'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with batch normalization\n",
    "def neural_net(x, weights, biases, keep_prob, train, reuse):\n",
    "    with tf.variable_scope('Neural_Net',reuse=reuse):\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_1 = tf.matmul(x, weights['h1'])\n",
    "        layer1_bn = tf.contrib.layers.batch_norm(inputs=layer_1,is_training=train,activation_fn=tf.nn.relu)\n",
    "        layer_1 = tf.nn.dropout(layer1_bn, keep_prob) #keep prob for dropout\n",
    "\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_2 = tf.matmul(layer_1, weights['h2'])\n",
    "        layer2_bn = tf.contrib.layers.batch_norm(inputs=layer_2,is_training=train,activation_fn=tf.nn.relu)\n",
    "        layer_2 = tf.nn.dropout(layer2_bn, keep_prob) #keep prob for dropout\n",
    "\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "predictions_train = neural_net(X, weights, biases, keep_prob, train=True, reuse=None)\n",
    "predictions_val = neural_net(X, weights, biases, keep_prob, train=False, reuse=True)\n",
    "\n",
    "#cost function is cross-entropy (sigmoid)\n",
    "cost_train = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions_train, labels=Y))\n",
    "cost_val = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions_val, labels=Y))\n",
    "\n",
    "loss_train_sum = tf.summary.scalar('train_loss',cost_train)\n",
    "loss_val_sum= tf.summary.scalar('val_loss',cost_val)\n",
    "\n",
    "#ACCURACY\n",
    "####################################################################\n",
    "pred_prob_tr = tf.nn.sigmoid(predictions_train)\n",
    "pred_bin_tr = tf.cast(pred_prob_tr > 0.5, dtype=tf.float32)\n",
    "correct_prediction_tr = tf.equal(pred_bin_tr, Y)\n",
    "correct_prediction_tr = tf.cast(correct_prediction_tr, tf.float32)\n",
    "accuracy_train = tf.reduce_mean(correct_prediction_tr)\n",
    "\n",
    "pred_prob_val = tf.nn.sigmoid(predictions_val)\n",
    "pred_bin_val = tf.cast(pred_prob_val > 0.5, dtype=tf.float32)\n",
    "correct_prediction_val = tf.equal(pred_bin_val, Y)\n",
    "correct_prediction_val = tf.cast(correct_prediction_val, tf.float32)\n",
    "accuracy_val = tf.reduce_mean(correct_prediction_val)\n",
    "\n",
    "acc_tr_sum = tf.summary.scalar('train_acc',accuracy_train)\n",
    "acc_val_sum= tf.summary.scalar('val_acc',accuracy_val)\n",
    "####################################################################\n",
    "\n",
    "#AUC\n",
    "####################################################################\n",
    "auc_tr, auc_tr_opt = tf.metrics.auc(Y, predictions=pred_prob_tr, name='auc_tr') #takes probs as an input\n",
    "auc_val, auc_val_opt = tf.metrics.auc(Y, predictions=pred_prob_val, name='auc_val') #takes probs as an input\n",
    "\n",
    "auc_tr_sum = tf.summary.scalar('train_auc',auc_tr_opt)\n",
    "auc_val_sum= tf.summary.scalar('val_auc',auc_val_opt)\n",
    "####################################################################\n",
    "\n",
    "#PRECISION\n",
    "####################################################################\n",
    "precision_tr, precision_tr_opt = tf.metrics.precision(Y, predictions=pred_bin_tr, name='precision_tr')\n",
    "precision_val, precision_val_opt = tf.metrics.precision(Y, predictions=pred_bin_val, name='precision_val')\n",
    "\n",
    "precision_tr_sum = tf.summary.scalar('train_precision',precision_tr_opt)\n",
    "precision_val_sum= tf.summary.scalar('val_precision',precision_val_opt)\n",
    "####################################################################\n",
    "\n",
    "#RECALL\n",
    "####################################################################\n",
    "recall_tr, recall_tr_opt = tf.metrics.recall(Y, predictions=pred_bin_tr, name='recall_tr')\n",
    "recall_val, recall_val_opt = tf.metrics.recall(Y, predictions=pred_bin_val, name='recall_val')\n",
    "\n",
    "recall_tr_sum = tf.summary.scalar('train_recall',recall_tr_opt)\n",
    "recall_val_sum= tf.summary.scalar('val_recall',recall_val_opt)\n",
    "####################################################################\n",
    "\n",
    "summary_ops_tr = tf.summary.merge([acc_tr_sum, loss_train_sum, auc_tr_sum, \n",
    "                                   precision_tr_sum, recall_tr_sum])\n",
    "summary_ops_val = tf.summary.merge([acc_val_sum, loss_val_sum, auc_val_sum, \n",
    "                                    precision_val_sum, recall_val_sum])\n",
    "\n",
    "#get collections grabs variables under the scope 'UPDATE_OPS'\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "logdir='/home/german/ESC_RNA_seq/pathway_enrichment_analysis/machine_learning/deep_learning/tf_event/'\n",
    "writer=tf.summary.FileWriter(logdir)\n",
    "\n",
    "# control_dependencies ensures updates are done before backpropagation\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_train,global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_local_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.local_variables_initializer` instead.\n",
      "Epoch: 0001 cost= 3.721027816\n",
      "Epoch: 0003 cost= 2.841066736\n",
      "Epoch: 0005 cost= 2.384701459\n",
      "Epoch: 0007 cost= 2.155918975\n",
      "Epoch: 0009 cost= 1.896588268\n",
      "Epoch: 0011 cost= 1.665088304\n",
      "Epoch: 0013 cost= 1.550280616\n",
      "Epoch: 0015 cost= 1.405730387\n",
      "Epoch: 0017 cost= 1.248929445\n",
      "Epoch: 0019 cost= 1.069665330\n",
      "Epoch: 0021 cost= 0.960680990\n",
      "Epoch: 0023 cost= 0.836676181\n",
      "Epoch: 0025 cost= 0.728346195\n",
      "Epoch: 0027 cost= 0.639900695\n",
      "Epoch: 0029 cost= 0.551424410\n",
      "Epoch: 0031 cost= 0.488612004\n",
      "Epoch: 0033 cost= 0.425053754\n",
      "Epoch: 0035 cost= 0.378174001\n",
      "Epoch: 0037 cost= 0.327302839\n",
      "Epoch: 0039 cost= 0.300845575\n",
      "Epoch: 0041 cost= 0.256918131\n",
      "Epoch: 0043 cost= 0.239602227\n",
      "Epoch: 0045 cost= 0.197784731\n",
      "Epoch: 0047 cost= 0.177083585\n",
      "Epoch: 0049 cost= 0.180875312\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "#EVALUATION\n",
    "Y_validation = np.reshape(Y_validation, (-1,1))\n",
    "#Y_train = np.reshape(Y_train, (-1,1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.initialize_local_variables())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(X_train) / batch_size)\n",
    "        x_batches = np.array_split(X_train, total_batch)\n",
    "        y_batches = np.array_split(Y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            batch_y = np.reshape(batch_y, (-1,1))\n",
    "            \n",
    "            _,train_sum, c,global_step_o = sess.run([optimizer,summary_ops_tr, cost_train,global_step], \n",
    "                            feed_dict={X : batch_x, \n",
    "                                       Y : batch_y, \n",
    "                                       keep_prob : 0.8})\n",
    "            writer.add_summary(train_sum,global_step_o)\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        sess.run(tf.initialize_local_variables())\n",
    "        val_sum, c_val = sess.run([summary_ops_val, cost_val], \n",
    "                            feed_dict={X : X_validation, \n",
    "                                       Y : Y_validation, \n",
    "                                       keep_prob : 1})\n",
    "        \n",
    "        writer.add_summary(val_sum,epoch)\n",
    "        #avg_cost += c / total_batch\n",
    "            \n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "\n",
    "    #print(\"Validation accuracy:\", accuracy.eval({X: X_validation, Y: Y_validation, keep_prob: 1.0}))\n",
    "    \n",
    "    #AUC\n",
    "    #nit = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    #sess.run(init)\n",
    "    #_,roc_score = tf.metrics.auc(labels = Y, predictions = pred_prob)\n",
    "    #print(roc_score.eval({X : X_validation, Y : Y_validation, keep_prob: 1.0}))\n",
    "    \n",
    "    #FOR CONFUSION MATRIX\n",
    "    x = pred_bin_val.eval({X : X_validation, Y : Y_validation, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE OF CONFUSION MATRIX IN TENSORFLOW\n",
    "y =  [1, 1, 1, 1, 1, 0, 0, 0]\n",
    "y_ = [1, 1, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "con = tf.confusion_matrix(labels=y, predictions=y_ )\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "        print(sess.run(con)) #ROWS ARE TRUE LABELS, COLUMNS ARE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1518   91]\n",
      " [  67   18]]\n"
     ]
    }
   ],
   "source": [
    "#CONFUSION MATRIX\n",
    "xr = x.reshape(1, 1694)\n",
    "yv = Y_validation.reshape(1, 1694)\n",
    "\n",
    "#print(xr.tolist()[0])\n",
    "#print(yv.tolist()[0])\n",
    "con = tf.confusion_matrix(labels=yv.tolist()[0], predictions=xr.tolist()[0])\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "        print(sess.run(con)) #ROWS ARE TRUE LABELS, COLUMNS ARE PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY:\n",
    "0.9067\n",
    "SENSITIVITY:\n",
    "0.364\n",
    "SPECIFICITY:\n",
    "0.9353\n",
    "PRECISION:\n",
    "0.2296\n",
    "F1:\n",
    "0.2818"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
