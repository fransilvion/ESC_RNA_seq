---
title: "Machine learning for drugs"
author: "German Novakovskiy"
date: "September 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library(knitr)
suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(GEOquery)))
suppressWarnings(suppressMessages(library(data.table)))
suppressWarnings(suppressMessages(library(reshape2)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(cowplot)))
suppressWarnings(suppressMessages(library(e1071)))
suppressWarnings(suppressMessages(library(tibble)))
suppressWarnings(suppressMessages(library(RColorBrewer)))
suppressWarnings(suppressMessages(library(edgeR)))
suppressWarnings(suppressMessages(library(readxl)))
suppressWarnings(suppressMessages(library(AUC)))
suppressMessages(suppressWarnings(library(randomForest)))
suppressMessages(suppressWarnings(library(hgu133a.db)))
suppressMessages(suppressWarnings(library(ReactomePA)))
suppressMessages(suppressWarnings(library(clusterProfiler)))
suppressMessages(suppressWarnings(library(fgsea)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rlist)))
suppressMessages(suppressWarnings(library(rcdk)))
suppressMessages(suppressWarnings(library(webchem)))
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(pROC)))
suppressMessages(suppressWarnings(library(glmnet)))
suppressMessages(suppressWarnings(library(ROCR)))
suppressMessages(suppressWarnings(library(logisticPCA)))
suppressMessages(suppressWarnings(library(keras)))
suppressMessages(suppressWarnings(library(onehot)))
```

Let's build several machine learning models for SOX17 activation prediction based on small molecule chemical structure. We will consider this problem as a binary classification: 0 if SOX17 is not activated, 1 otherwise. Activation of SOX17 will be based on corresponding log fold change from LINCS expression data. 

SOX17 is a major marker of definitive endoderm, it's highly expressed at this stage and it's expression is downregulated during pluripotency stage. 

This is how distribution of SOX17 expression looks like:
```{r}
load("MCF7_gene_expression.Rdata")

sox17_lfc <- rows_cols_gene_expression_mcf7["SOX17",]
#for testing
foxa2_lfc <- rows_cols_gene_expression_mcf7["FOXA2",]
rm(rows_cols_gene_expression_mcf7)

hist(sox17_lfc, breaks=50, main = "SOX17 logFC distribution",
     xlab = "logFC", col = "cyan")
```

Looks like a standard npormal distribution (it should be we log fold changes). We will take 5% (95% percentile) at the right tail as activated genes:
```{r}
#what is 95% percentile
threshold <- quantile(sox17_lfc, c(.95))
threshold
```

Expression of SOX17 in interesting drugs from KEGG/Wiki analysis: 
```{r}
drugs <- c("taxifolin", "SA-1462738", "SA-1938111", "SA-1938219", "SA-247654", "SA-418993", "SA-85901", "SA-89705", "SA-1459008", "BRD-K20168484", "carbetocin", "linifanib", "mestanolone")

sox17_lfc[drugs][sox17_lfc[drugs] > threshold]
```

Only 5 out of 13 molecules!

The same for FOXA2:
```{r}
foxa2_lfc[drugs][foxa2_lfc[drugs] > quantile(foxa2_lfc, c(.95))] #1.007819 
rm(foxa2_lfc)
```
Only 3 molecules are common between these two sets -> SA-1938111, SA-247654, mestanolone

```{r}
#drugs that activate expression of SOX17 
drugs_positive <- sox17_lfc[sox17_lfc >= threshold] #532 drugs

#drugs that do not activate expression of SOX17 (we do not care about inactivation)
drugs_negative <- sox17_lfc[sox17_lfc < threshold] #10095 drugs
```

We will use MACCS small molecule descriptors - binary in nature and typically encode the presence or absence of substructural fragments (calculate them with Rcpi package). 
```{r}
#function for retrieving canonical SMILES from PubChem
#input - drug name
#output - SMILES
getSmiles <- function(drug_name, df_drugs){
  my_drug <- df_drugs %>% filter(pert_iname == drug_name)
  
  #if someone is not 666
  if (nrow(my_drug) > 1){ 
    if (my_drug %>% filter(pubchem_cid != -666) %>% nrow() == 1){
      my_drug <- my_drug %>% filter(pubchem_cid != -666) 
    }
    else if (my_drug %>% filter(is_touchstone != 0) %>% nrow() == 1){
      #if both 666 take with is_touchstone 1
      my_drug <- my_drug %>% filter(is_touchstone != 0)
    } else {
      #like in case of CHIR-99021
      my_drug <- my_drug[1,]
    }
    }
  
  res <- as.character(my_drug$canonical_smiles)
  if (res == -666 | res == 'restricted') { res <- NULL}
  return(res)
}

#function for retrieving MACCS 166 bit descriptors
#input - SMILES
#output - MACCS 166 descriptors
getMACCS <- function(smiles_character){
  mols <- parse.smiles(smiles_character)
  fp <- get.fingerprint(mols[[1]], type="maccs")
  
  #fp is a S4 class, bits field is a vector, indicating the bit positions that are on.
  res <- fp@bits
  return(res)
}
```

Load LINCS drug information (from GSE92742_Broad_LINCS_pert_info.txt.gz)
```{r}
#cols - "pert_id", "pert_iname", "pert_type", "is_touchstone", "inchi_key_prefix", "inchi_key", "canonical_smiles","pubchem_cid" 
drug_info <- read.table("LINCS_compounds_info.txt", header = T, quote = "", sep = "\t", comment.char = "")
dim(drug_info)
```
Note that there are repeats, for example "trichostatin-a" has 2 entries. In this case we take only those, for which pubchem_cid does not equal -666. (however, the one line for this drug with -666 has classical smiles, while another one with pubchem id has isomeric smiles...).
```{r}
#how many drugs among positives have repeats in drug_info file
x <- drug_info %>% filter(pert_iname %in% names(drugs_positive)) 
length(x$pert_iname[which(duplicated(x$pert_iname))])
```
```{r}
#those replicates
as.character(x$pert_iname[which(duplicated(x$pert_iname))])
```
```{r}
#how many drugs among negatives have repeats in drug_info file
x <- drug_info %>% filter(pert_iname %in% names(drugs_negative)) 
length(x$pert_iname[which(duplicated(x$pert_iname))])
```

Note that some of the drugs in info file don't have Smiles. We simply delete them. 

Try to run for some drugs and measure time:
```{r}
#takes ~5 seconds
for_testing <- names(drugs_positive)
start.time <- Sys.time()

drugs_smiles_positive <- sapply(for_testing, getSmiles, drug_info)

#delete zeros if there are some
x <- lapply(drugs_smiles_positive, length)
deleted_positives <- sum(x == 0)
drugs_smiles_positive <- drugs_smiles_positive[which(x != 0)]
###############################

drug_maccs_positive <- sapply(drugs_smiles_positive, getMACCS)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
#takes ~1 minute
for_testing <- names(drugs_negative)
start.time <- Sys.time()
 
drugs_smiles_negative <- sapply(for_testing, getSmiles, drug_info)

#delete zeros if there are some
x <- lapply(drugs_smiles_negative, length)
deleted_negatives <- sum(x == 0)
drugs_smiles_negative <- drugs_smiles_negative[-which(x == 0)]
###############################

drug_maccs_negative <- sapply(drugs_smiles_negative, getMACCS)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
length(drug_maccs_positive)
```
```{r}
length(drug_maccs_negative)
```
```{r}
#function for turning the maccs bit descriptors to binary vector with 166 terms
bitsToVector <- function(non_zero_indexes){
  y <- rep(0, 166)
  y[non_zero_indexes] = 1
  return(y)
}

drug_maccs_positive <- lapply(drug_maccs_positive, bitsToVector)

drug_maccs_negative <- lapply(drug_maccs_negative, bitsToVector)

#converting lists to data frame
drugs <- names(drug_maccs_positive)
drug_maccs_positive <- data.frame(matrix(unlist(drug_maccs_positive), nrow=532, byrow=T))
rownames(drug_maccs_positive) <- drugs

drugs <- names(drug_maccs_negative)
drug_maccs_negative <- data.frame(matrix(unlist(drug_maccs_negative), nrow=10057, byrow=T))
rownames(drug_maccs_negative) <- drugs

#for correlation estimates
test <- rbind(drug_maccs_positive, drug_maccs_negative)
test_for_corr <- test[,colSums(test) > 0] #there are 7 bits in 166-vector that are all 0
cormat <- round(cor(test_for_corr), 3)
melted_cormat <- melt(cormat)

melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```

```{r}
#adding variavle Inducer - 1 (if yes), 0 (if not)
drug_maccs_positive$Inducer <- rep("Yes", nrow(drug_maccs_positive))

drug_maccs_negative$Inducer <- rep("No", nrow(drug_maccs_negative))

save(drug_maccs_positive, file = "drug_maccs_positive.Rdata")
save(drug_maccs_negative, file = "drug_maccs_negative.Rdata")
```

Now we can try to perform actual machine learning algorithms for this problem. Note that we have an imbalanced class problem here (10:0.5). Thus we have to take this into account, this [link](https://www.r-bloggers.com/handling-class-imbalance-with-r-and-caret-an-introduction/) was used as a reference. We use class weights to handle class imbalance. 

For all machine learning models use tuneLength = 10!

## Logistic regression

We start with weighted logistic regression:
```{r}
all_data <- rbind(drug_maccs_positive, drug_maccs_negative)
all_data$Inducer <- factor(all_data$Inducer)

prop.table(table(all_data$Inducer)) #0 is not an inducer, 1 is an inducer (of SOX17 expression)
```

Performing logistic PCA:
```{r}
Inducer <- all_data$Inducer
for_pca <- all_data[,-167]
logsvd_model <- logisticSVD(for_pca, k = 2)
logsvd_model
```
```{r}
logpca_cv = cv.lpca(for_pca, ks = 2, ms = 1:10)
save(logpca_cv, file = "logpca_cv.Rdata")
plot(logpca_cv)
```
```{r}
plot(logsvd_model, type = "scores") + geom_point(aes(colour = Inducer, size = Inducer, alpha = Inducer)) + 
  ggtitle("Exponential Family PCA") + scale_colour_manual(values = c("blue", "red"))
```


```{r}
logpca_model = logisticPCA(for_pca, k = 2, m = which.min(logpca_cv))

for_pca_cleaned <- for_pca[, colSums(for_pca) > 0]
clogpca_model = convexLogisticPCA(for_pca_cleaned, k = 2, m = which.min(logpca_cv))

plot(logpca_model, type = "scores") + geom_point(aes(colour = Inducer, size = Inducer, alpha = Inducer)) + 
  ggtitle("Logistic PCA") + scale_colour_manual(values = c("blue", "red"))
```
```{r}
plot(clogpca_model, type = "scores") + geom_point(aes(colour = Inducer, size = Inducer, alpha = Inducer)) + 
  ggtitle("Convex Logistic PCA") + scale_colour_manual(values = c("blue", "red"))
```


Using this [link](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/) as a reference.
```{r}
#deleting columns with zeros everywhere
x <- all_data$Inducer
all_data <- all_data[,-167]
all_data <- all_data[,colSums(all_data) > 0]
all_data$Inducer <- x

#splitting into train and test sets (using stratified shuffle split to retain class proportions)
train.index <- createDataPartition(all_data$Inducer, p = 0.8, list = FALSE)
train_data <- all_data[train.index,]
test_data <- all_data[-train.index,]

save(train_data, file = "train_data.Rdata")
save(test_data, file = "test_data.Rdata")

load("train_data.Rdata")
load("test_data.Rdata")

#get the validation set for deep learning (see below)
validation.index <- createDataPartition(train_data$Inducer, p = 0.8, list = FALSE)
train_data_deep <- train_data[validation.index,]
validation_data_deep <- train_data[-validation.index,]

save(train_data_deep, file = "train_data_deep.Rdata")
save(validation_data_deep, file = "validation_data_deep.Rdata")

load("train_data_deep.Rdata")
load("validation_data_deep.Rdata")

###########################################
#FOR TESTING
#x <- test_data$Inducer
#test_data <- test_data[,-167]
#test_data <- test_data[,colSums(test_data) > 0]
#test_data$Inducer <- x
#rm(x)
#train.index <- createDataPartition(test_data$Inducer, p = 0.8, list = FALSE)
#train_test <- test_data[train.index,]
#test_test <- test_data[-train.index,]
###########################################

# Create model weights (they sum to one) - check in caret documentation!!
model_weights <- ifelse(train_data$Inducer == "No",
                        (1/table(train_data$Inducer)[1])*0.5,
                        (1/table(train_data$Inducer)[2])*0.5)

# Set up control function for training (stratified k-fold with repeats)
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)#, sampling="up")

# Build weighted model (no regularizer) - gives ideal predictions
#weighted_logistic_fit <- train(Inducer ~ .,
#                               data = train_data,
#                               method = "glm",
#                               weights = model_weights,
#                               metric = "ROC",
#                               trControl = ctrl)

# Build model with lasso regularizer (alpha 1 stands for lasso; .5 for elastic net, 0 for ridge regression)
set.seed(1492)

tuneGrid=expand.grid(
              .alpha=1,
              .lambda=seq(0, 2, by = 0.1))


weighted_logistic_fit_lasso <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl)
save(weighted_logistic_fit_lasso, file="weighted_logistic_fit_lasso.Rdata")
# Build model with elastic regularizer (alpha 1 stands for lasso; .5 for elastic net, 0 for ridge regression)
set.seed(1492)

tuneGrid=expand.grid(
              .alpha=0.5,
              .lambda=seq(0, 2, by = 0.1))


weighted_logistic_fit_elastic <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl)

save(weighted_logistic_fit_elastic, file="weighted_logistic_fit_elastic.Rdata")
# Build model with ridge regularizer (alpha 1 stands for lasso; .5 for elastic net, 0 for ridge regression)
set.seed(1492)

tuneGrid=expand.grid(
              .alpha=0,
              .lambda=seq(0, 10, by = 0.1))


weighted_logistic_fit_ridge <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl)

save(weighted_logistic_fit_ridge, file="weighted_logistic_fit_ridge.Rdata")
# Build custom AUC function to extract AUC
# from the caret model object
test_roc <- function(model, data) {
  
  pROC::roc(data$Inducer,
      predict(model, data, type = "prob")[, "Yes"])

}

model_list <- list(lasso = weighted_logistic_fit_lasso,
                   elastic = weighted_logistic_fit_elastic,
                   ridge = weighted_logistic_fit_ridge)


model_list_roc <- model_list %>%
  map(test_roc, data = test_data)

model_list_roc %>%
  map(pROC::auc)

#weighted_logistic_fit_lasso %>%
#  test_roc(data = test_data) %>%
#  pROC::auc()
weighted_logistic_fit_ridge
```
```{r}
rValues <- resamples(model_list)
bwplot(rValues, metric="ROC")
```

## K nearest neighbors

Try to follow this [link](https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/) for now.
```{r}
#it takes 15 minutes to run, be careful (for tuneLength = 3 - k=5,7,9; for k=9 ROC is 0.77 with 0.99 sens, but 0.012 specific)
#running with expand.grid(k = seq(8,20,by=2)) takes half an hour (31 min)
set.seed(1492)

start.time <- Sys.time()

tuneGrid <- expand.grid(k = seq(8,26,by=2))

knn_model_fit <- train(Inducer ~ .,
                       data = train_data,
                       method = "knn",
                       #weights = model_weights,
                       #family = "binomial",
                       metric = "ROC",
                       tuneGrid = tuneGrid, 
                       #tuneLength=10,
                       trControl = ctrl)

save(knn_model_fit, file="knn_model_fit.Rdata")

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
knn_model_fit$results
```


```{r}
plot(knn_model_fit)
```
```{r}
valid_pred <- predict(knn_model_fit, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}
knn_model_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
rValues <- resamples(list(ridge = weighted_logistic_fit_ridge,
                          knn = knn_model_fit))
bwplot(rValues, metric="ROC")
```


## Support Vector Machines

(Try Gaussian kernel - radial basis function)
We receive 'Cannot scale data' because some of the columns are all zeros, and that is why we don't do pre-processing: preProcess default - no pre-processing.  
```{r}
set.seed(1492)
#takes 5 minutres
start.time <- Sys.time()

#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))

svm_Linear <- train(Inducer ~., 
                    data = train_data, 
                    method = "svmLinear",
                    trControl=ctrl,
                    #tuneLength = 9,
                    metric = "ROC", scale=FALSE)

save(svm_Linear, file="svm_Linear.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
svm_Linear %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
valid_pred <- predict(svm_Linear, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}

rValues <- resamples(list(ridge = weighted_logistic_fit_ridge,
                          knn = knn_model_fit,
                          svm_linear = svm_Linear))
bwplot(rValues, metric="ROC")
```

```{r}
#with Gausian kernel
#runs almost 3 hours!!!
set.seed(1492)
start.time <- Sys.time()

svm_Gaussian <- train(Inducer ~., 
                    data = train_data, 
                    method = "svmRadial",
                    trControl=ctrl,
                    metric = "ROC",
                    #tuneGrid = tuneGrid,
                    tuneLength = 10,
                    scale = FALSE)

save(svm_Gaussian, file="svm_Gaussian.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

```

```{r}
svm_Gaussian
```

```{r}
svm_Gaussian %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
valid_pred <- predict(svm_Gaussian, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}
rValues <- resamples(list(ridge = weighted_logistic_fit_ridge,
                          knn = knn_model_fit,
                          svm_linear = svm_Linear,
                          svm_gaussian = svm_Gaussian))
bwplot(rValues, metric="ROC")
```

## SVM (e1071)

```{r}
#runs for 10 hours!!! params ara cost - 0.1, gamma - 1
set.seed(1492)

start.time <- Sys.time()

#test_train_data <- lapply(train_data, factor)
#test_train_data <- as.data.frame(test_train_data)

#tc <- tune.control(cross = 5, nrepeat = 5)
#x_train <- as.matrix(test_train_data[,1:159])

svm_tune <- tune(svm, train.x=train_data[,1:159], train.y=train_data$Inducer, 
              kernel="radial", ranges=list(cost=10^(-1:3), gamma=c(0.1,0.5,1,2,4)), tunecontrol = tc,
              class.weights= c("No" = 1, "Yes" = 19), scale = FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

svm_tune
```
```{r}
set.seed(1492)

start.time <- Sys.time()

svm_model_after_tune <- svm(Inducer ~ ., data=train_data, kernel="radial", cost=0.1, gamma=1,
                            class.weights= c("No" = 1, "Yes" = 19), scale = FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
save(svm_model_after_tune, file = "svm_model_after_tune.Rdata")

# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(svm_model_after_tune, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```


## Naive bayes

Naive bayes with weights:
```{r}
set.seed(1492)
#it takes 1 hour to run
start.time <- Sys.time()

#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))

#test_train_data <- test_train_data[ , !(names(test_train_data) %in% c("X8", "X11", "X137"))]

test_train_data <- lapply(train_data, factor)
test_train_data <- as.data.frame(test_train_data)

tuneGrid <- expand.grid(fL = c(1),
                        usekernel = c(FALSE),
                        adjust=c(1.0))

nbayes_fit <- train(Inducer ~., 
                    data = test_train_data, 
                    method = "nb",
                    trControl=ctrl,
                    weights = model_weights,
                    tuneGrid = tuneGrid, 
                    #tuneLength = 10,
                    metric = "ROC")
                    #preProcess = "conditionalX")

save(nbayes_fit, file="nb_fit.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

nbayes_fit
```
```{r}
nbayes_fit %>%
  test_roc(data = train_data[1:1000,]) %>%
  pROC::auc()
```
```{r}
# make predictions
x_test <- train_data[1:1000,1:159]
y_test <- train_data[1:1000,]$Inducer
predictions <- predict(nbayes_fit, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(nbayes_fit, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```


## Random Forest

Random forest from randomForest package with strata and samp sizes
```{r}
set.seed(1492)
start.time <- Sys.time()

tuneGrid <- expand.grid(mtry = 2^(1:4))

#1000 trees, uch that most observations participate in several trees (ideally it would be 2000)
randomforest_fit <- train(Inducer ~., 
                    data = train_data, 
                    method = "rf",
                    trControl=ctrl,
                    #weights = model_weights,
                    #tuneLength = 10,
                    tuneGrid = tuneGrid, strata = train_data$Inducer,
                    sampsize = rep(sum(train_data$Inducer == "Yes"), 2),
                    metric = "ROC", ntree = 2000, allowParallel=TRUE)


save(randomforest_fit, file="randomforest_fit.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
randomforest_fit
```
```{r}
randomforest_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(randomforest_fit, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```


Just classic random forest:
```{r} 
set.seed(1492)
#it takes 1 hour to run
start.time <- Sys.time()

#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))

rf_fit <- train(Inducer ~., 
                    data = train_data, 
                    method = "rf",
                    trControl=ctrl,
                    #weights = model_weights,
                    tuneLength = 10,
                    metric = "ROC",
                    allowParallel=TRUE)

save(rf_fit, file="rf_fit.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
rf_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
#mtry (#Randomly Selected Predictors)
rf_fit$results
```
```{r}
valid_pred <- predict(rf_fit, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}
rValues <- resamples(list(ridge = weighted_logistic_fit_ridge,
                          knn = knn_model_fit,
                          svm_linear = svm_Linear,
                          svm_gaussian = svm_Gaussian,
                          rf = rf_fit))
bwplot(rValues, metric="ROC")
```

Random forest with weights:
```{r}
set.seed(1492)
#takes 33 minutes to run!
start.time <- Sys.time()
#it's around 500 trees by default...
#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))
tuneGrid <- expand.grid(mtry = c(8,12,20),
                        splitrule = c("gini", "extratrees"),
                        min.node.size = 1)

rf_weighted_fit <- train(Inducer ~., 
                    data = train_data, 
                    method = "ranger",
                    trControl=ctrl,
                    tuneLength = 5,
                    weights = model_weights,
                    metric = "ROC")

rf_weighted_fit_tuned <- train(Inducer ~., 
                    data = train_data, 
                    method = "ranger",
                    trControl=ctrl,
                    #tuneLength = 5,
                    tuneGrid = tuneGrid,
                    weights = model_weights,
                    metric = "ROC")

save(rf_weighted_fit, file="rf_weighted_fit.Rdata")
#still mtry = 2 is the best...
save(rf_weighted_fit, file="rf_weighted_fit_tuned.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
rf_weighted_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
#mtry (#Randomly Selected Predictors)
rf_weighted_fit
```
```{r}
valid_pred <- predict(rf_weighted_fit, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```


## Stochastic Gradient boosting

```{r}
set.seed(1492)
#takes two hours to run!
start.time <- Sys.time()

#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))

gbm_fit <- train(Inducer ~., 
                    data = train_data, 
                    method = "gbm",
                    trControl=ctrl,
                    weights = model_weights,
                    metric = "ROC",
                    tuneLength = 5, verbose = FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

save(gbm_fit, file="gbm_fit.Rdata")

gbm_fit
```

```{r}
gbm_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}

valid_pred <- predict(gbm_fit, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}
rValues <- resamples(list(ridge = weighted_logistic_fit_ridge,
                          knn = knn_model_fit,
                          svm_linear = svm_Linear,
                          svm_gaussian = svm_Gaussian,
                          rf = rf_fit,
                          rf_weighted = rf_weighted_fit,
                          boosting = gbm_fit))
bwplot(rValues, metric = "ROC")

```

# REFINING

We will refine logistic regression with ridge and stochastic gradient boosting to see if we can increase the performance.

## Logistic regression with ridge

```{r}
weighted_logistic_fit_ridge
```
The optimal lambda is 0.1. Let's play around this value a little bit:
```{r}
set.seed(1492)

tuneGrid=expand.grid(
              .alpha=0,
              .lambda=seq(0.05, 0.2, by = 0.01))


weighted_logistic_fit_ridge_tuned <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl)

weighted_logistic_fit_ridge_tuned
```

it's tiny improvement in ~0.0001
```{r}
weighted_logistic_fit_ridge_tuned %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```

## Gradient boosting

Now let's focus on improving stochastic gradient boosting (gbm):
```{r}
gbm_fit
```

Let's tune this a little bit:
```{r}
set.seed(1492)
#takes two hours to run!
start.time <- Sys.time()

#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))
tuneGrid <- expand.grid(n.trees = seq(40,100,by=10),
                        interaction.depth = c(3,4,5),
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

gbm_fit_tuned <- train(Inducer ~., 
                    data = train_data, 
                    method = "gbm",
                    trControl=ctrl,
                    weights = model_weights,
                    metric = "ROC",
                    tuneGrid = tuneGrid,
                    #tuneLength = 5, 
                    verbose = FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

```
```{r}
gbm_fit_tuned
```
```{r}
gbm_fit_tuned %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```

# Accuracy metric

Since improvement is not so big, we will focus on three best methods so far: logistic regression with ridge, stochastic gradient boosting, random forest. And check the accuracy metric:

## Logistic regression with ridge

```{r}
set.seed(1492)

tuneGrid=expand.grid(
              .alpha=0,
              .lambda=0.1) #the optimal

ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 5)

weighted_logistic_fit_ridge_accuracy <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "Accuracy",
                                     trControl = ctrl)

save(weighted_logistic_fit_ridge_accuracy, file="weighted_logistic_fit_ridge_accuracy.Rdata")

weighted_logistic_fit_ridge_accuracy
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(weighted_logistic_fit_ridge_accuracy, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
Accuracy is 0.752
```{r}
accuracy <- table(predictions, y_test)
sum(diag(accuracy))/sum(accuracy)
```

## Random forest

```{r}
set.seed(1492)
#takes 33 minutes to run!
start.time <- Sys.time()

tuneGrid=expand.grid(
              mtry=2,
              splitrule='extratrees',
              min.node.size=1) #the optimal


rf_weighted_fit_accuracy <- train(Inducer ~., 
                    data = train_data, 
                    method = "ranger",
                    trControl=ctrl,
                    tuneGrid = tuneGrid,
                    weights = model_weights,
                    metric = "Accuracy")

save(rf_weighted_fit_accuracy, file="rf_weighted_fit_accuracy.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

rf_weighted_fit_accuracy
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(rf_weighted_fit_accuracy, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```

## Stochastic gradient boosting

```{r}
set.seed(1492)
#takes two hours to run!
start.time <- Sys.time()

tuneGrid <- expand.grid(n.trees = 50,
                        interaction.depth = 4,
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

gbm_fit_accuracy <- train(Inducer ~., 
                    data = train_data, 
                    method = "gbm",
                    trControl=ctrl,
                    weights = model_weights,
                    metric = "Accuracy",
                    tuneGrid = tuneGrid, 
                    verbose = FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

save(gbm_fit_accuracy, file="gbm_fit_accuracy.Rdata")
gbm_fit_accuracy
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(gbm_fit_accuracy, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
```{r}
rValues <- resamples(list(ridge = weighted_logistic_fit_ridge_accuracy,
                          rf_weighted = rf_weighted_fit_accuracy,
                          boosting = gbm_fit_accuracy))
bwplot(rValues, metric="Accuracy")
```

Let's calculate accuracy for other classifiers (knn, svm) to show their accuracy

## KNN

```{r}
set.seed(1492)

start.time <- Sys.time()

tuneGrid <- expand.grid(k = 26)

knn_model_fit_accuracy <- train(Inducer ~ .,
                       data = train_data,
                       method = "knn",
                       #weights = model_weights,
                       #family = "binomial",
                       metric = "Accuracy",
                       tuneGrid = tuneGrid, 
                       #tuneLength=10,
                       trControl = ctrl)

save(knn_model_fit_accuracy, file="knn_model_fit_accuracy.Rdata")

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

knn_model_fit_accuracy
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(knn_model_fit_accuracy, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```

## SVM linear

```{r}
set.seed(1492)
#takes 5 minutres
start.time <- Sys.time()

tuneGrid <- expand.grid(C = 1)

svm_Linear_accuracy <- train(Inducer ~., 
                    data = train_data, 
                    method = "svmLinear",
                    trControl=ctrl,
                    #tuneLength = 9,
                    metric = "Accuracy", 
                    scale=FALSE)

save(svm_Linear_accuracy, file="svm_Linear_accuracy.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

svm_Linear_accuracy
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(svm_Linear_accuracy, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```

## SVM Gaussian 

```{r}
#with Gausian kernel
#runs almost 3 hours!!!
set.seed(1492)
start.time <- Sys.time()

tuneGrid <- expand.grid(sigma = 0.0045,
                        C = 128)

svm_Gaussian_accuracy <- train(Inducer ~., 
                    data = train_data, 
                    method = "svmRadial",
                    trControl=ctrl,
                    metric = "Accuracy",
                    tuneGrid = tuneGrid,
                    #tuneLength = 10,
                    scale = FALSE)

save(svm_Gaussian_accuracy, file="svm_Gaussian_accuracy.Rdata")
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

svm_Gaussian_accuracy
```
```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(svm_Gaussian_accuracy, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
##Neural nets

###Model Averaged Neural Network
```{r, message=FALSE, warning=FALSE}
set.seed(1492)

#tuneGrid=expand.grid(
#              .alpha=1,
#              .lambda=seq(0, 2, by = 0.1))
start.time <- Sys.time()

weighted_avnnet <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "avNNet",
                                     weights = model_weights,
                                     #family = "binomial",
                                     #tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl, verbose = FALSE, MaxNWts = 2000)


end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
save(weighted_avnnet, file="weighted_avnnet.Rdata")

weighted_avnnet %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(weighted_avnnet, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
```{r}
weighted_avnnet
```

```{r}

set.seed(1492)

#tuneGrid=expand.grid(
#              .alpha=1,
#              .lambda=seq(0, 2, by = 0.1))
start.time <- Sys.time()

weighted_nnet <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "nnet",
                                     weights = model_weights,
                                     #family = "binomial",
                                     #tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl, verbose = FALSE, MaxNWts = 2000)


end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
save(weighted_nnet, file="weighted_nnet.Rdata")


weighted_nnet %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(weighted_nnet, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
```{r}
weighted_nnet
```

```{r}
set.seed(1492)

#tuneGrid=expand.grid(
#              .alpha=1,
#              .lambda=seq(0, 2, by = 0.1))
start.time <- Sys.time()

weighted_pcaNNet <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "pcaNNet",
                                     weights = model_weights,
                                     #family = "binomial",
                                     #tuneGrid = tuneGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl, verbose = FALSE, MaxNWts = 2000)


end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
save(weighted_pcaNNet, file="weighted_pcaNNet.Rdata")


weighted_pcaNNet %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(weighted_pcaNNet, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```

```{r}
#tune model parameters using cross-validation
set.seed(1492)

#tuneGrid=expand.grid(
#              .alpha=1,
#              .lambda=seq(0, 2, by = 0.1))
start.time <- Sys.time()

#nnetGrid <-  expand.grid(size = seq(from = 9, to = 12, by = 1),
#                        decay = 1*10^(-4:-1))

weighted_nnet_tune <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "nnet",
                                     weights = model_weights,
                                     #family = "binomial",
                                     #tuneGrid = nnetGrid,
                                     tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl, verbose = FALSE, MaxNWts = 2500)


end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
weighted_nnet <- weighted_nnet_tune
save(weighted_nnet, file="weighted_nnet.Rdata")
weighted_nnet

#weighted_nnet %>%
  #test_roc(data = test_data) %>%
  #pROC::auc()
```

```{r}
nnetGrid <-  expand.grid(size = seq(from = 9, to = 12, by = 1),
                        decay = 1*10^(-4:-1))

weighted_nnet_tune <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "nnet",
                                     weights = model_weights,
                                     #family = "binomial",
                                     tuneGrid = nnetGrid,
                                     #tuneLength=5,
                                     metric = "ROC",
                                     trControl = ctrl, verbose = FALSE, MaxNWts = 2500)

save(weighted_nnet_tune, file="weighted_nnet_tune.Rdata")
weighted_nnet_tune
```

```{r}
gbm_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```

```{r}
# make predictions
x_test <- test_data[,1:159]
y_test <- test_data$Inducer
predictions <- predict(gbm_fit, x_test)
# summarize results
confusionMatrix(predictions, y_test)

```

## Deep neural nets

Let's try to fit deep neural network using keras:
```{r}
write.table(train_data, file = "train_data.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(test_data, file = "test_data.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)


x_train <- train_data[,1:159]
y_train <- train_data$Inducer
x_train_deep <- train_data_deep[,1:159]
y_train_deep <- train_data_deep$Inducer
x_validation_deep <- validation_data_deep[,1:159]
y_validation_deep <- validation_data_deep$Inducer

x_test <- test_data[,1:159]
y_test <- test_data$Inducer

#for one hot encoding
encoder_train_deep <- onehot(as.data.frame(y_train_deep))
y_train_deep_onehot <- predict(encoder_train_deep, as.data.frame(y_train_deep))
encoder_validation_deep <- onehot(as.data.frame(y_validation_deep))
y_validation_deep_onehot <- predict(encoder_validation_deep, as.data.frame(y_validation_deep))
encoder_test <- onehot(as.data.frame(y_test))
y_test_onehot <- predict(encoder_test, as.data.frame(y_test))

y_train <- as.character(y_train)
y_train[y_train == "Yes"] <-  1
y_train[y_train == "No"] <-  0
y_train <- factor(y_train, levels = c(0,1))

y_train_deep <- as.character(y_train_deep)
y_train_deep[y_train_deep == "Yes"] <-  1
y_train_deep[y_train_deep == "No"] <-  0
y_train_deep <- factor(y_train_deep, levels = c(0,1))

y_validation_deep <- as.character(y_validation_deep)
y_validation_deep[y_validation_deep == "Yes"] <-  1
y_validation_deep[y_validation_deep == "No"] <-  0
y_validation_deep <- factor(y_validation_deep, levels = c(0,1))

y_test <- as.character(y_test)
y_test[y_test == "Yes"] <-  1
y_test[y_test == "No"] <-  0
y_test <- factor(y_test, levels = c(0,1))

x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
dimnames(x_train) <- NULL

x_train_deep <- as.matrix(x_train_deep)
y_train_deep <- as.matrix(y_train_deep)
dimnames(x_train_deep) <- NULL
dimnames(y_train_deep_onehot) <- NULL

x_test <- as.matrix(x_test)
y_test <- as.matrix(y_test)
dimnames(x_test) <- NULL

x_validation_deep <- as.matrix(x_validation_deep)
y_validation_deep <- as.matrix(y_validation_deep)
dimnames(x_validation_deep) <- NULL
dimnames(y_validation_deep_onehot) <- NULL
dimnames(y_test_onehot) <- NULL

#y_trainLabels <- to_categorical(y_train)
#y_trainLabels_deep <- to_categorical(y_train_deep)

# One hot encode test target values
#y_testLabels <- to_categorical(y_test)

#Oversampling with onehot
all_train <- cbind(x_train_deep, y_train_deep_onehot)
all_train_yes <- all_train[1:341,]
new_dat_train <- all_train_yes[sample(nrow(all_train_yes), 6000, replace = T),]
all_train <- rbind(new_dat_train, all_train)
x_train_deep_over <- all_train[,1:159]
y_train_deep_onehot_over <- all_train[,160:161]

#Oversampling without onehot
all_train <- cbind(x_train_deep, y_train_deep)
all_train_yes <- all_train[1:341,]
new_dat_train <- all_train_yes[sample(nrow(all_train_yes), 6000, replace = T),]
all_train <- rbind(new_dat_train, all_train)
x_train_deep_over <- all_train[,1:159]
y_train_deep_over <- all_train[,160]
y_train_deep_over <- as.data.frame(y_train_deep_over)

write.table(x_train, file = "x_train.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(x_test, file = "x_test.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_train, file = "y_train.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_test, file = "y_test.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(x_train_deep, file = "x_train_deep.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_train_deep, file = "y_train_deep.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(x_validation_deep, file = "x_validation_deep.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_validation_deep, file = "y_validation_deep.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

#for one hot
write.table(y_train_deep_onehot, file = "y_train_deep_onehot.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_validation_deep_onehot, file = "y_validation_deep_onehot.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_test_onehot, file = "y_test_onehot.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

#oversampling
write.table(x_train_deep_over, file = "x_train_deep_over.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_train_deep_over, file = "y_train_deep_over.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

write.table(y_train_deep_onehot_over, file = "y_train_deep_onehot_over.txt", "quote" = FALSE,
          sep = "\t", row.names = FALSE, col.names = FALSE)

```

We have a lot of hyperparameters here:

* Learning rate
* Momentum beta
* Mini-batch size
* No. of hidden units
* No. of layers
* Learning rate decay
* Regularization lambda
* Activation functions
* Adam beta1 & beta2

Just let's start with some random model:
```{r}
#model <- keras_model_sequential() 

# Add layers to the model
#we have 159 features
#model %>% 
#    layer_dense(units = 10, activation = 'relu', input_shape = c(159)) %>% 
#    layer_dense(units = 5, activation = 'relu') %>%
#    layer_dense(units = 1, activation = 'sigmoid')

#model %>% compile(
#     loss = 'binary_crossentropy',
#     optimizer = 'adam',
#     metrics = 'accuracy'
# )

#counter <- funModeling::freq(y_train, plot=F) %>% dplyr::select(var, frequency)
#majority <- max(counter$frequency)
#counter$weight <- ceiling(majority/counter$frequency)

#l_weights <- setNames(as.list(counter$weight), counter$var)

# Save the training history in the history variable
#There are 95% drugs which are class 0, and 5% drugs which are class 0.  
#I balance 2 classes using the class_weights
#history <- model %>% fit(
#  x_train, y_train, 
#  epochs = 200, batch_size = 256, 
#  validation_split = 0.2,
#  class_weight = list(0.95, 0.05)
# )

# Plot the model loss of the training data
#plot(history$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", col="blue", type="l")

# Plot the model loss of the test data
#lines(history$metrics$val_loss, col="green")

# Add legend
#legend("topright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
```

```{r}
# Plot the accuracy of the training data 
#plot(history$metrics$acc, main="Model Accuracy", xlab = "epoch", ylab="accuracy", col="blue", type="l")

# Plot the accuracy of the validation data
#lines(history$metrics$val_acc, col="green")

# Add Legend
#legend("bottomright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
```
```{r}
# Predict the classes for the test data
#classes <- model %>% predict_classes(x_test, batch_size = 128)

# Confusion matrix
#table(y_test, classes)
```

Python:
```{python, engine.path = '/usr/bin/python3'}
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
import numpy
from collections import Counter
from sklearn.metrics import classification_report, confusion_matrix
# fix random seed for reproducibility
numpy.random.seed(7)

X_train = numpy.loadtxt("x_train.txt", delimiter="\t")
Y_train = numpy.loadtxt("y_train.txt", delimiter="\t")
X_test = numpy.loadtxt("x_test.txt", delimiter="\t")
Y_test = numpy.loadtxt("y_test.txt", delimiter="\t")

#class1_train = X_train[0:426, :]
#class0_train = X_train[426:, :]
#new_class1 = numpy.random.randint(low=0,high=len(class1_train), size = 8000)
#new_class1 = class1_train[new_class1]
#X_train = numpy.concatenate((new_class1, class0_train))

#Y0_train = Y_train[426:] 
#Y1_train = numpy.repeat(1, 8000)
#Y_train = numpy.concatenate((Y1_train, Y0_train))

def get_class_weights(y):
    counter = Counter(y)
    majority = max(counter.values())
    return  {cls: float(majority/count) for cls, count in counter.items()}
    
l_weights = get_class_weights(Y_train)

# create model
model = Sequential()
model.add(Dense(120, input_dim=159, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

#compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


#history = model.fit(X_train, Y_train, epochs=50, batch_size=512, validation_split = 0.2)
history = model.fit(X_train, Y_train, epochs=50, batch_size=10, validation_split = 0.2, class_weight = {0:1, 1:19})

# summarize history for loss
#plt.plot(history.history['loss'])
#plt.plot(history.history['val_loss'])
#plt.title('model loss')
#plt.ylabel('loss')
#plt.xlabel('epoch')
#plt.legend(['train', 'test'], loc='upper left')
#plt.show()

# summarize history for accuracy
#plt.plot(history.history['acc'])
#plt.plot(history.history['val_acc'])
#plt.title('model accuracy')
#plt.ylabel('accuracy')
#plt.xlabel('epoch')
#plt.legend(['train', 'test'], loc='upper left')
#plt.show()
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)
cm = confusion_matrix(Y_test, y_pred)
print(cm)
```


Python code with Tensorflow (add batch normalization and initialize with Xavier-glorot):
```{python, engine.path = '/usr/bin/python3'}
from __future__ import print_function
import tensorflow as tf
import numpy as np

#read the data
#X_train = np.loadtxt("x_train_deep.txt", delimiter="\t")
#Y_train = np.loadtxt("y_train_deep.txt", delimiter="\t")

#OVERSAMPLING
X_train = np.loadtxt("x_train_deep_over.txt", delimiter="\t")
Y_train = np.loadtxt("y_train_deep_over.txt", delimiter="\t")

X_validation = np.loadtxt("x_validation_deep.txt", delimiter="\t")
Y_validation = np.loadtxt("y_validation_deep.txt", delimiter="\t")
#Y_validation = np.loadtxt("y_validation_deep_onehot.txt", delimiter="\t")

X_test = np.loadtxt("x_test.txt", delimiter="\t")
Y_test = np.loadtxt("y_test.txt", delimiter="\t")
#Y_test = np.loadtxt("y_test_onehot.txt", delimiter="\t")

print("X train dim: ", X_train.shape)
print("Y train dim: ", Y_train.shape)
print("X validation dim: ", X_validation.shape)
print("Y validation dim: ", Y_validation.shape)
print("X test dim: ", X_test.shape)
print("Y test dim: ", Y_test.shape)

#to make things reproducible
random_state = 42
np.random.seed(random_state)
tf.set_random_seed(random_state)

# HyperParameters
training_epochs = 150
display_step = 2
batch_size = 64
learning_rate = 0.001
keep_prob = 0.8 #dropout 

# Network Parameters
n_hidden_1 = 256 # 1st layer number of neurons
n_hidden_2 = 256 # 2nd layer number of neurons
#n_hidden_3 = 256
num_input = 159 # MACCS descriptors without 7 features
num_classes = 2 #  total classes (Inducer/not-Inducer); they are mutually exclusive

# tf Graph input
X = tf.placeholder("float", [None, num_input])
Y = tf.placeholder("float", [None, 1]) 

# Store layers weight & bias
weights = {
    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    #'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),
    'out': tf.Variable(tf.random_normal([n_hidden_2, 1]))
    #'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))
}
biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'b2': tf.Variable(tf.random_normal([n_hidden_2])),
    #'b3': tf.Variable(tf.random_normal([n_hidden_3])),
    'out': tf.Variable(tf.random_normal([1]))
    #'out': tf.Variable(tf.random_normal([num_classes]))
}


keep_prob = tf.placeholder("float")

# Create model
def neural_net(x, weights, biases, keep_prob):
    # Hidden fully connected layer with 256 neurons
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.relu(layer_1) #relu function
    layer_1 = tf.nn.dropout(layer_1, keep_prob) #keep prob for dropout
    # Hidden fully connected layer with 256 neurons
    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
    layer_2 = tf.nn.relu(layer_2) #relu function
    layer_2 = tf.nn.dropout(layer_2, keep_prob) #keep prob for dropout
    
    #layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])
    #layer_3 = tf.nn.relu(layer_3) #relu function
    #layer_3 = tf.nn.dropout(layer_3, keep_prob) #keep prob for dropout
    # Output fully connected layer with a neuron for each class
    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] 
    #out_layer = tf.matmul(layer_3, weights['out']) + biases['out']
    return out_layer
    
#create model
predictions = neural_net(X, weights, biases, keep_prob)

#cost function is cross-entropy (sigmoid)
cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions, labels=Y))
#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=Y))

#use Adam optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

#EVALUATION
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for epoch in range(training_epochs):
        avg_cost = 0.0
        total_batch = int(len(X_train) / batch_size)
        x_batches = np.array_split(X_train, total_batch)
        y_batches = np.array_split(Y_train, total_batch)
        for i in range(total_batch):
            batch_x, batch_y = x_batches[i], y_batches[i]
            batch_y = np.reshape(batch_y, (-1,1))
            _, c = sess.run([optimizer, cost], 
                            feed_dict={X : batch_x, 
                                       Y : batch_y, 
                                       keep_prob : 0.8})
            avg_cost += c / total_batch
        if epoch % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", \
                "{:.9f}".format(avg_cost))

    print("Optimization Finished!")
    
    #ACCURACY
    # make into a probability
    pred_prob = tf.nn.sigmoid(predictions)
    # threshold above 0.5, or use AUC
    pred_bin = tf.cast(pred_prob > 0.5, dtype=tf.float32)
    
    correct_prediction = tf.equal(pred_bin, Y)
    correct_prediction = tf.cast(correct_prediction, tf.float32)
    accuracy = tf.reduce_mean(correct_prediction)
    Y_validation = np.reshape(Y_validation, (-1,1))
    print("Validation accuracy:", accuracy.eval({X: X_validation, Y: Y_validation, keep_prob: 1.0}))
    
    #AUC
    #a = tf.placeholder(tf.float32)
    #b = tf.placeholder(tf.float32)
    
    #a = tf.cast(tf.argmax(pred_prob, 1), "float")
    #b = tf.cast(tf.argmax(Y,1), "float")
    #init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    #sess.run(init)
    #_,roc_score = tf.metrics.auc(b, a)
    #print(roc_score.eval({X : X_validation, Y : Y_validation, keep_prob: 1.0}))
    #FOR CONFUSION MATRIX
    x = pred_bin.eval({X : X_validation, Y : Y_validation, keep_prob: 1.0})

#CONFUSION MATRIX with validation set
xr = x.reshape(1, 1694)
yv = Y_validation.reshape(1, 1694)

#print(xr.tolist()[0])
#print(yv.tolist()[0])
con = tf.confusion_matrix(labels=yv.tolist()[0], predictions=xr.tolist()[0])
sess = tf.Session()
with sess.as_default():
        print(sess.run(con)) #ROWS ARE TRUE LABELS, COLUMNS ARE PREDICTIONS
```



