---
title: "Machine learning for drugs"
author: "German Novakovskiy"
date: "September 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library(knitr)
suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(GEOquery)))
suppressWarnings(suppressMessages(library(data.table)))
suppressWarnings(suppressMessages(library(reshape2)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(cowplot)))
suppressWarnings(suppressMessages(library(limma)))
suppressWarnings(suppressMessages(library(tibble)))
suppressWarnings(suppressMessages(library(RColorBrewer)))
suppressWarnings(suppressMessages(library(edgeR)))
suppressWarnings(suppressMessages(library(readxl)))
suppressWarnings(suppressMessages(library(VennDiagram)))
suppressMessages(suppressWarnings(library(ermineR)))
suppressMessages(suppressWarnings(library(hgu133a.db)))
suppressMessages(suppressWarnings(library(ReactomePA)))
suppressMessages(suppressWarnings(library(clusterProfiler)))
suppressMessages(suppressWarnings(library(fgsea)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rlist)))
suppressMessages(suppressWarnings(library(rcdk)))
suppressMessages(suppressWarnings(library(webchem)))
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(pROC)))
suppressMessages(suppressWarnings(library(glmnet)))
suppressMessages(suppressWarnings(library(ROCR)))
```

Let's build several machine learning models for SOX17 activation prediction based on small molecule chemical structure. We will consider this problem as a binary classification: 0 if SOX17 is not activated, 1 otherwise. Activation of SOX17 will be based on corresponding log fold change from LINCS expression data. 

SOX17 is a major marker of definitive endoderm, it's highly expressed at this stage and it's expression is downregulated during pluripotency stage. 

This is how distribution of SOX17 expression looks like:
```{r}
load("MCF7_gene_expression.Rdata")

sox17_lfc <- rows_cols_gene_expression_mcf7["SOX17",]
rm(rows_cols_gene_expression_mcf7)

hist(sox17_lfc, breaks=50, main = "SOX17 logFC distribution",
     xlab = "logFC", col = "cyan")
```

Looks like a standard npormal distribution (it should be we log fold changes). We will take 5% (95% percentile) at the right tail as activated genes:
```{r}
#what is 95% percentile
threshold <- quantile(sox17_lfc, c(.95))
threshold
```
```{r}
#drugs that activate expression of SOX17 
drugs_positive <- sox17_lfc[sox17_lfc >= threshold] #532 drugs

#drugs that do not activate expression of SOX17 (we do not care about inactivation)
drugs_negative <- sox17_lfc[sox17_lfc < threshold] #10095 drugs
```

We will use MACCS small molecule descriptors - binary in nature and typically encode the presence or absence of substructural fragments (calculate them with Rcpi package). 
```{r}
#function for retrieving canonical SMILES from PubChem
#input - drug name
#output - SMILES
getSmiles <- function(drug_name, df_drugs){
  my_drug <- df_drugs %>% filter(pert_iname == drug_name)
  
  #if someone is not 666
  if (nrow(my_drug) > 1){ 
    if (my_drug %>% filter(pubchem_cid != -666) %>% nrow() == 1){
      my_drug <- my_drug %>% filter(pubchem_cid != -666) 
    }
    else if (my_drug %>% filter(is_touchstone != 0) %>% nrow() == 1){
      #if both 666 take with is_touchstone 1
      my_drug <- my_drug %>% filter(is_touchstone != 0)
    } else {
      #like in case of CHIR-99021
      my_drug <- my_drug[1,]
    }
    }
  
  res <- as.character(my_drug$canonical_smiles)
  if (res == -666 | res == 'restricted') { res <- NULL}
  return(res)
}

#function for retrieving MACCS 166 bit descriptors
#input - SMILES
#output - MACCS 166 descriptors
getMACCS <- function(smiles_character){
  mols <- parse.smiles(smiles_character)
  fp <- get.fingerprint(mols[[1]], type="maccs")
  
  #fp is a S4 class, bits field is a vector, indicating the bit positions that are on.
  res <- fp@bits
  return(res)
}
```

Load LINCS drug information (from GSE92742_Broad_LINCS_pert_info.txt.gz)
```{r}
#cols - "pert_id", "pert_iname", "pert_type", "is_touchstone", "inchi_key_prefix", "inchi_key", "canonical_smiles","pubchem_cid" 
drug_info <- read.table("LINCS_compounds_info.txt", header = T, quote = "", sep = "\t", comment.char = "")
dim(drug_info)
```
Note that there are repeats, for example "trichostatin-a" has 2 entries. In this case we take only those, for which pubchem_cid does not equal -666. (however, the one line for this drug with -666 has classical smiles, while another one with pubchem id has isomeric smiles...).
```{r}
#how many drugs among positives have repeats in drug_info file
x <- drug_info %>% filter(pert_iname %in% names(drugs_positive)) 
length(x$pert_iname[which(duplicated(x$pert_iname))])
```
```{r}
#those replicates
as.character(x$pert_iname[which(duplicated(x$pert_iname))])
```
```{r}
#how many drugs among negatives have repeats in drug_info file
x <- drug_info %>% filter(pert_iname %in% names(drugs_negative)) 
length(x$pert_iname[which(duplicated(x$pert_iname))])
```

Note that some of the drugs in info file don't have Smiles. We simply delete them. 

Try to run for some drugs and measure time:
```{r}
#takes ~5 seconds
for_testing <- names(drugs_positive)
start.time <- Sys.time()

drugs_smiles_positive <- sapply(for_testing, getSmiles, drug_info)

#delete zeros if there are some
x <- lapply(drugs_smiles_positive, length)
deleted_positives <- sum(x == 0)
drugs_smiles_positive <- drugs_smiles_positive[which(x != 0)]
###############################

drug_maccs_positive <- sapply(drugs_smiles_positive, getMACCS)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
#takes ~1 minute
for_testing <- names(drugs_negative)
start.time <- Sys.time()
 
drugs_smiles_negative <- sapply(for_testing, getSmiles, drug_info)

#delete zeros if there are some
x <- lapply(drugs_smiles_negative, length)
deleted_negatives <- sum(x == 0)
drugs_smiles_negative <- drugs_smiles_negative[-which(x == 0)]
###############################

drug_maccs_negative <- sapply(drugs_smiles_negative, getMACCS)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
length(drug_maccs_positive)
```
```{r}
length(drug_maccs_negative)
```
```{r}
#function for turning the maccs bit descriptors to binary vector with 166 terms
bitsToVector <- function(non_zero_indexes){
  y <- rep(0, 166)
  y[non_zero_indexes] = 1
  return(y)
}

drug_maccs_positive <- lapply(drug_maccs_positive, bitsToVector)

drug_maccs_negative <- lapply(drug_maccs_negative, bitsToVector)

#converting lists to data frame
drugs <- names(drug_maccs_positive)
drug_maccs_positive <- data.frame(matrix(unlist(drug_maccs_positive), nrow=532, byrow=T))
rownames(drug_maccs_positive) <- drugs

drugs <- names(drug_maccs_negative)
drug_maccs_negative <- data.frame(matrix(unlist(drug_maccs_negative), nrow=10057, byrow=T))
rownames(drug_maccs_negative) <- drugs

#for correlation estimates
test <- rbind(drug_maccs_positive, drug_maccs_negative)
test_for_corr <- test[,colSums(test) > 0] #there are 7 bits in 166-vector that are all 0
cormat <- round(cor(test_for_corr), 3)
melted_cormat <- melt(cormat)

melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```

```{r}
#adding variavle Inducer - 1 (if yes), 0 (if not)
drug_maccs_positive$Inducer <- rep("Yes", nrow(drug_maccs_positive))

drug_maccs_negative$Inducer <- rep("No", nrow(drug_maccs_negative))

save(drug_maccs_positive, file = "drug_maccs_positive.Rdata")
save(drug_maccs_negative, file = "drug_maccs_negative.Rdata")
```

Now we can try to perform actual machine learning algorithms for this problem. Note that we have an imbalanced class problem here (10:0.5). Thus we have to take this into account, this [link](https://www.r-bloggers.com/handling-class-imbalance-with-r-and-caret-an-introduction/) was used as a reference. We use class weights to handle class imbalance. 

## Logistic regression

We start with weighted logistic regression:
```{r}
all_data <- rbind(drug_maccs_positive, drug_maccs_negative)
all_data$Inducer <- factor(all_data$Inducer)

prop.table(table(all_data$Inducer)) #0 is not an inducer, 1 is an inducer (of SOX17 expression)
```

Using this [link](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/) as a reference.
```{r}
#splitting into train and test sets (using stratified shuffle split to retain class proportions)
train.index <- createDataPartition(all_data$Inducer, p = 0.8, list = FALSE)
train_data <- all_data[train.index,]
test_data <- all_data[-train.index,]

# Create model weights (they sum to one) - check in caret documentation!!
model_weights <- ifelse(train_data$Inducer == "No",
                        (1/table(train_data$Inducer)[1])*0.5,
                        (1/table(train_data$Inducer)[2])*0.5)

# Set up control function for training (stratified k-fold with repeats)
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Build weighted model (no regularizer) - gives ideal predictions
#weighted_logistic_fit <- train(Inducer ~ .,
#                               data = train_data,
#                               method = "glm",
#                               weights = model_weights,
#                               metric = "ROC",
#                               trControl = ctrl)

# Build model with lasso regularizer (alpha 1 stands for lasso; .5 for elastic net, 0 for ridge regression)
tuneGrid=expand.grid(
              .alpha=1,
              .lambda=seq(0, 10, by = 0.1))


weighted_logistic_fit_lasso <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     metric = "ROC",
                                     trControl = ctrl)

# Build model with elastic regularizer (alpha 1 stands for lasso; .5 for elastic net, 0 for ridge regression)
tuneGrid=expand.grid(
              .alpha=0.5,
              .lambda=seq(0, 10, by = 0.1))


weighted_logistic_fit_elastic <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     metric = "ROC",
                                     trControl = ctrl)

# Build model with ridge regularizer (alpha 1 stands for lasso; .5 for elastic net, 0 for ridge regression)
tuneGrid=expand.grid(
              .alpha=0,
              .lambda=seq(0, 10, by = 0.1))


weighted_logistic_fit_ridge <- train(Inducer ~ .,
                                     data = train_data,
                                     method = "glmnet",
                                     weights = model_weights,
                                     family = "binomial",
                                     tuneGrid = tuneGrid,
                                     metric = "ROC",
                                     trControl = ctrl)

# Build custom AUC function to extract AUC
# from the caret model object
test_roc <- function(model, data) {
  
  pROC::roc(data$Inducer,
      predict(model, data, type = "prob")[, "Yes"])

}

model_list <- list(lasso = weighted_logistic_fit_lasso,
                   elastic = weighted_logistic_fit_elastic,
                   ridge = weighted_logistic_fit_ridge)


model_list_roc <- model_list %>%
  map(test_roc, data = test_data)

model_list_roc %>%
  map(pROC::auc)

#weighted_logistic_fit_lasso %>%
#  test_roc(data = test_data) %>%
#  pROC::auc()
```

## K nearest neighbors

Try to follow this [link](https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/) for now.
```{r}
#it takes 15 minutes to run, be careful (for tuneLength = 3 - k=5,7,9; for k=9 ROC is 0.77 with 0.99 sens, but 0.012 specific)
#running with expand.grid(k = seq(8,20,by=2)) takes half an hour (31 min)
start.time <- Sys.time()

tuneGrid <- expand.grid(k = seq(8,20,by=2))

knn_model_fit <- train(Inducer ~ .,
                       data = train_data,
                       method = "knn",
                       #weights = model_weights,
                       #family = "binomial",
                       metric = "ROC",
                       tuneGrid = tuneGrid, 
                       trControl = ctrl)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
knn_model_fit
```


```{r}
plot(knn_model_fit)
```
```{r}
valid_pred <- predict(knn_model_fit, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}
knn_model_fit %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```

## Support Vector Machines

(Try Gaussian kernel - radial basis function)
We receive 'Cannot scale data' because some of the columns are all zeros, and that is why we don't do pre-processing: preProcess default - no pre-processing.  
```{r}
#takes 5 minutres
start.time <- Sys.time()

#tuneGrid <- expand.grid(C = seq(0.1,10,by=0.1))

svm_Linear <- train(Inducer ~., 
                    data = train_data, 
                    method = "svmLinear",
                    trControl=ctrl,
                    metric = "ROC", scale=FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```
```{r}
svm_Linear %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
valid_pred <- predict(svm_Linear, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```
```{r}
svm_Linear
```

```{r}
#with Gausian kernel
#runs almost 3 hours!!!
start.time <- Sys.time()

tuneGrid <- expand.grid(sigma = c(.01, .015, 0.2),
                        C = c(0.75, 0.9, 1, 1.1, 1.25))

svm_Gaussian <- train(Inducer ~., 
                    data = train_data, 
                    method = "svmRadial",
                    trControl=ctrl,
                    metric = "ROC",
                    tuneGrid = tuneGrid, scale = FALSE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

save(svm_Gaussian, file = "svm_Gaussian.Rdata")
```

```{r}
svm_Gaussian
```

```{r}
svm_Gaussian %>%
  test_roc(data = test_data) %>%
  pROC::auc()
```
```{r}
valid_pred <- predict(svm_Gaussian, test_data, type = "prob")

pred_val <-prediction(valid_pred[,2], test_data$Inducer)

#perf_val <- performance(pred_val, "auc") #0.81

perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 2.5)
```

## Random Forest

Try bagging for ensemble.

## Naive Bayes

## Extra tree (ETs)??